{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>ERA5 Weather Data </u>\n",
    "\n",
    "[ERA5](https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation) is the fifth generation ECMWF reanalysis for the global climate and weather for the the period from January 1950 onwards published by the Copernicus Climate Change Service. The ERA5 comprises [several datasets](https://confluence.ecmwf.int/display/CKB/The+family+of+ERA5+datasets) which provide estimates for a large number of atmospheric, ocean-wave and land-surface quantities. Reanalysis data combine information from ground stations, satellites, weather balloons, and other inputs with a climate model to estimate weather variables across a grid. \n",
    "\n",
    "## Time reference\n",
    "\n",
    "Time reference is tricky because ERA5 data covers the entire globe with different time zones. Specifically, ERA5 data run from 00 UTC (Coordinated Universal Time). This means all variables refer to *UTC+/-00:00*. The US (exluding Alaska and Hawaii) counties have different time zones ranging from UTC-5 to UTC-8.\n",
    "\n",
    "<img src=\"./Shapefiles/time_zones/Time_Zones.png\" style=\"height:500px\" /> \n",
    "\n",
    "| Time Zone | Longitude | Latitude |\n",
    "| :- | :-: | :-: |\n",
    "| UTC-5 | (-91,-65) | (23,50)\n",
    "| UTC-6 | (-106,-83) | (24,51)\n",
    "| UTC-7 | (-120,-99) | (29,51) \n",
    "| UTC-8 | (-126,-113) | (31,51)\n",
    "\n",
    "Thus, **daily weather data must be download for each time zone seperately**. The Climate Data Store (CDS) provides an [ERA5 API](https://cds.climate.copernicus.eu/api-how-to). The API allows to specify the coordinate window, UTC-time and aggregation level (daily mean, max or min) (see [PDF Table 2](https://datastore.copernicus-climate.eu/documents/app-c3s-daily-era5-statistics/C3S_Application-Documentation_ERA5-daily-statistics-v2.pdf) for details or this [example code](https://confluence.ecmwf.int/pages/viewpage.action?pageId=228867588)).\n",
    "\n",
    "\n",
    "## <u> ERA5 hourly (on single level) data </u>\n",
    "\n",
    "**[ERA5 hourly data](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview) on single levels from 1979 to present**. We will use **daily data** for **different time zones (UTC-5 to UTC-8)** by averaging hourly data with a horizontal resolution of **0.25°x0.25°** for the period **2006 to 2020**. Specifically, the following weather variables are imported:\n",
    "* Boundary layer height in m\n",
    "* Total cloud cover from 0-1 (This parameter is the proportion of a grid box covered by cloud)\n",
    "\n",
    "Following code downloads ERA5 daily averages for each US time zone (UTC with respective coordinate window) though the ERA5 API. Each netCDF file contains data for one variable per month and UTC coordinate window shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import cdsapi\n",
    "import requests\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# PATH\n",
    "PATH = \"C:/Users/u0120816/OneDrive - KU Leuven/FB/Data/Python/ERA5_single_levels/\"\n",
    " \n",
    "# Requires:\n",
    "# 1) the CDS API to be installed and working on your system\n",
    "# 2) You have agreed to the ERA5 Licence (via the CDS web page)\n",
    "# 3) Selection of required variable, daily statistic, etc\n",
    "\n",
    "# Call API\n",
    "c = cdsapi.Client(timeout=1000)\n",
    "\n",
    "# Time Zones\n",
    "UTC =  [\"UTC-05\", \"UTC-06\", \"UTC-07\", \"UTC-08\"]\n",
    "\n",
    "# Variables\n",
    "VAR =  [\"boundary_layer_height\", \"total_cloud_cover\"]\n",
    "\n",
    "# Years\n",
    "YEARS =  [\n",
    "  '2006',  '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016',  '2017', '2018', '2019', '2020'\n",
    "]\n",
    "\n",
    "# Months\n",
    "MONTHS = [\n",
    "    '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'\n",
    "    ]\n",
    "\n",
    "# Loop over all parameters\n",
    "for yr in YEARS:\n",
    "    for mn in MONTHS:\n",
    "        for var in VAR:\n",
    "            for utc in UTC:\n",
    "                \n",
    "                print('Running: '+yr+mn+var+utc)        \n",
    "                \n",
    "                #--- UTC-05 ---#                \n",
    "\n",
    "                if utc == \"UTC-05\":\n",
    "                    result = c.service(\n",
    "                        \"tool.toolbox.orchestrator.workflow\",\n",
    "                        params={\n",
    "                            \"realm\": \"user-apps\",\n",
    "                            \"project\": \"app-c3s-daily-era5-statistics\",\n",
    "                            \"version\": \"master\",\n",
    "                            \"kwargs\": {\n",
    "                            \"dataset\": \"reanalysis-era5-single-levels\",\n",
    "                            \"product_type\": \"reanalysis\",\n",
    "                            \"variable\": var,\n",
    "                            \"statistic\": \"daily_mean\",\n",
    "                            \"year\": yr,\n",
    "                            \"month\": mn,\n",
    "                            \"time_zone\": utc+\":0\",\n",
    "                            \"frequency\": \"1-hourly\",\n",
    "                            \"grid\": \"0.25/0.25\",                        \n",
    "                            \"area\": {\"lat\": [23,50], \"lon\": [-91,-65]}   \n",
    "                    },\n",
    "                    \"workflow_name\": \"application\"\n",
    "                })\n",
    "\n",
    "                #--- UTC-06 ---#\n",
    "\n",
    "                if utc == \"UTC-06\":\n",
    "                    result = c.service(\n",
    "                        \"tool.toolbox.orchestrator.workflow\",\n",
    "                        params={\n",
    "                            \"realm\": \"user-apps\",\n",
    "                            \"project\": \"app-c3s-daily-era5-statistics\",\n",
    "                            \"version\": \"master\",\n",
    "                            \"kwargs\": {\n",
    "                            \"dataset\": \"reanalysis-era5-single-levels\",\n",
    "                            \"product_type\": \"reanalysis\",\n",
    "                            \"variable\": var,\n",
    "                            \"statistic\": \"daily_mean\",\n",
    "                            \"year\": yr,\n",
    "                            \"month\": mn,\n",
    "                            \"time_zone\": utc+\":0\",\n",
    "                            \"frequency\": \"1-hourly\",\n",
    "                            \"grid\": \"0.25/0.25\",\n",
    "                            \"area\": {\"lat\": [24,51], \"lon\": [-106,-83]}    \n",
    "                    },\n",
    "                    \"workflow_name\": \"application\"\n",
    "                })\n",
    "\n",
    "                #--- UTC-07 ---#\n",
    "\n",
    "                if utc == \"UTC-07\":\n",
    "                    result = c.service(\n",
    "                        \"tool.toolbox.orchestrator.workflow\",\n",
    "                        params={\n",
    "                            \"realm\": \"user-apps\",\n",
    "                            \"project\": \"app-c3s-daily-era5-statistics\",\n",
    "                            \"version\": \"master\",\n",
    "                            \"kwargs\": {\n",
    "                            \"dataset\": \"reanalysis-era5-single-levels\",\n",
    "                            \"product_type\": \"reanalysis\",\n",
    "                            \"variable\": var,\n",
    "                            \"statistic\": \"daily_mean\",\n",
    "                            \"year\": yr,\n",
    "                            \"month\": mn,\n",
    "                            \"time_zone\": utc+\":0\",\n",
    "                            \"frequency\": \"1-hourly\",\n",
    "                            \"grid\": \"0.25/0.25\",\n",
    "                            \"area\": {\"lat\": [29,51], \"lon\": [-120,-99]}    \n",
    "                    },\n",
    "                    \"workflow_name\": \"application\"\n",
    "                })\n",
    "\n",
    "                #--- UTC-08 ---#\n",
    "\n",
    "                if utc == \"UTC-08\":\n",
    "                    result = c.service(\n",
    "                        \"tool.toolbox.orchestrator.workflow\",\n",
    "                        params={\n",
    "                            \"realm\": \"user-apps\",\n",
    "                            \"project\": \"app-c3s-daily-era5-statistics\",\n",
    "                            \"version\": \"master\",\n",
    "                            \"kwargs\": {\n",
    "                            \"dataset\": \"reanalysis-era5-single-levels\",\n",
    "                            \"product_type\": \"reanalysis\",\n",
    "                            \"variable\": var,\n",
    "                            \"statistic\": \"daily_mean\",\n",
    "                            \"year\": yr,\n",
    "                            \"month\": mn,\n",
    "                            \"time_zone\": utc+\":0\",\n",
    "                            \"frequency\": \"1-hourly\",\n",
    "                            \"grid\": \"0.25/0.25\",\n",
    "                            \"area\": {\"lat\": [31,51], \"lon\": [-126,-113]}    \n",
    "                    },\n",
    "                    \"workflow_name\": \"application\"\n",
    "                })\n",
    "                \n",
    "                # set name of output file for each month (statistic, variable, year, month)\n",
    "                file_name = \"download_\"+utc+\"_\" + var +\"_\"+ yr + mn+ \".nc\"                     \n",
    "                location=result[0]['location']    \n",
    "                res = requests.get(location, stream = True)\n",
    "                print(\"Writing data to \" + file_name)\n",
    "                with open(PATH+file_name,'wb') as fh:\n",
    "                    for r in res.iter_content(chunk_size = 1024):\n",
    "                        fh.write(r)\n",
    "                fh.close()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all data has been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Path with downloaded files\n",
    "PATH = \"E:/ERA5/US/ERA5_single_levels/\"\n",
    "\n",
    "# Generate list of files\n",
    "files = []\n",
    "df_files = pd.DataFrame(columns=['Files'])\n",
    "for file in sorted(os.listdir(PATH)):\n",
    "    if \".nc\" in file:\n",
    "        files.append(file)    \n",
    "df_files['Files']=files\n",
    "df_files['Actual_files']=1\n",
    "\n",
    "\n",
    "# Generate full list of files that should have been downloaded\n",
    "names = []\n",
    "UTC =  [\"UTC-05\", \"UTC-06\", \"UTC-07\", \"UTC-08\"]\n",
    "VAR =  [\"boundary_layer_height\", \"total_cloud_cover\"]\n",
    "for y in range(2006, 2020):\n",
    "    for m in range(1, 13):\n",
    "        for var in VAR:\n",
    "            for utc in UTC:       \n",
    "                d =\"download_\"+utc+\"_\" + var +\"_\"+ str(y) + str(m).zfill(2)+ \".nc\" \n",
    "                names.append(d)\n",
    "df_names = pd.DataFrame(names,  columns=['Files'])\n",
    "df_names['Required_files']=1\n",
    "\n",
    "# Compare the two lists and export missing files to CSV\n",
    "df = pd.merge(df_files, df_names, on='Files', how='outer')\n",
    "missings = df[df['Actual_files'].isna()].sort_values(by=['Files'])\n",
    "missings.to_csv(PATH + \"missings.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggreagte ERA5 single level data on county level\n",
    "The following script aggreates ERA5 hourly (on single levels) data over each US county taking the different time zones into account. \n",
    "In a first step, ERA5 single level data will be cleaned. ERA5 single level data is only available on a horizontal resolution of 0.25° x 0.25° which is to coarse to cover smaller counties. Thus, the data will be interpolated to finer grid of is 0.05°x0.05°. I will apply *bilinear interpolation* for the *boundary layer height* and *nearest neighbor interpolation* for *total cloud cover*. In a second step, the interpolated daily weather data will be aggreagted for each US county taking different time-zones into account. To do so, seperate shapefiles for each US timezone have been generated. Hourly weather data are aggregated for each time-zone-shapefile.\n",
    "\n",
    "1. **Interpolate data** to finer grid and store in new netcdf files. This task can be done on regular computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import sys\n",
    "import xarray as xr \n",
    "import numpy as np\n",
    "import regionmask\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "import dask\n",
    "\n",
    "# Path (ECOOM SERVER)\n",
    "PATH_INPUT = \"C:/Users/u0120816/Documents/Project_1/ERA5/\"\n",
    "PATH_OUTPUT = \"C:/Users/u0120816/Documents/Project_1/ERA5/Cleaned/\"\n",
    "\n",
    "# Start time\n",
    "start = time.time()  \n",
    "\n",
    "# Store filenames in text file for later use\n",
    "w = open(PATH_OUTPUT+'Filenames_2000_2020.txt', \"a\")\n",
    "w.write('Filename' + '\\n')\n",
    "\n",
    "# Time Zones\n",
    "UTC =  [\"UTC-05\", \"UTC-06\", \"UTC-07\", \"UTC-08\"]\n",
    "\n",
    "# Loop over all parameters\n",
    "for yr in range(2006, 2007):    \n",
    "    for mn in range(1, 13): \n",
    "        for utc in UTC:      \n",
    "            \n",
    "            try:\n",
    "\n",
    "                # Open Boundary layer heigth\n",
    "                d_blh = xr.open_mfdataset(PATH_INPUT+\"download_\"+utc+\"_boundary_layer_height_\"+ str(yr) + str(mn).zfill(2)+ \".nc\")                \n",
    "                d_blh = d_blh.rename({'lon':'longitude'})\n",
    "                d_blh = d_blh.rename({'lat':'latitude'})\n",
    "                \n",
    "                # Interpolate data to finer spatial resolution\n",
    "                # ERA5 gridded at 0.25 x 0.25 \n",
    "                # Interpolate by increasing long/lat by factor 25 leading to grid of 0.25/5 x 0.25/5 which is 0.05x0.05\n",
    "                new_lon = np.linspace(d_blh.longitude[0], d_blh.longitude[-1], d_blh.dims[\"longitude\"] * 5)\n",
    "                new_lat = np.linspace(d_blh.latitude[0], d_blh.latitude[-1], d_blh.dims[\"latitude\"] * 5)\n",
    "\n",
    "                # Bilinear for blh\n",
    "                d_blh_int = d_blh.interp(latitude=new_lat).interp(longitude=new_lon)\n",
    "\n",
    "                # Open cloud coverage\n",
    "                 # Open Boundary layer heigth\n",
    "                d_tcc = xr.open_mfdataset(PATH_INPUT+\"download_\"+utc+\"_total_cloud_cover_\"+str(yr) + str(mn).zfill(2)+ \".nc\")\n",
    "                d_tcc = d_tcc.rename({'lon':'longitude'})\n",
    "                d_tcc = d_tcc.rename({'lat':'latitude'})\n",
    "\n",
    "                # Interpolate data to finer spatial resolution\n",
    "                # ERA5 gridded at 0.25 x 0.25 \n",
    "                # Interpolate by increasing long/lat by factor 25 leading to grid of 0.25/5 x 0.25/5 which is 0.05x0.05\n",
    "                new_lon = np.linspace(d_tcc.longitude[0], d_tcc.longitude[-1], d_tcc.dims[\"longitude\"] * 5)\n",
    "                new_lat = np.linspace(d_tcc.latitude[0], d_tcc.latitude[-1], d_tcc.dims[\"latitude\"] * 5)\n",
    "\n",
    "                # Nearest Neighbor Interpolation for cloud coverage\n",
    "                d_tcc_int = d_tcc.interp(latitude=new_lat, method=\"nearest\").interp(longitude=new_lon, method=\"nearest\")\n",
    "\n",
    "                # Store\n",
    "                df = d_blh_int.combine_first(d_tcc_int)                \n",
    "                df.to_netcdf(PATH_OUTPUT+ 'ERA5_single_'+utc+\"_\"+ str(yr) + str(mn).zfill(2)+ \".nc\")\n",
    "                w.write('ERA5_single_'+utc+\"_\"+ str(yr) + str(mn).zfill(2)+ \".nc\" + '\\n')                \n",
    "\n",
    "            except:\n",
    "                continue   \n",
    "               \n",
    "\n",
    "w.close()\n",
    "print('Done!')\n",
    "end = time.time()\n",
    "print('Total Time: {} min'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Aggregate interpolated data on county level**. Data aggregation are done by running multiple python scripts in parallel. Each python script aggregates weather data of one month. The following code generates multiple python scripts to clean data for an entire year. To do so, I first generate a python script containing the base code. Following, multiple python scripts are generated by replacing file names (e.g. ERA5_single_UTC-05_200601). Multiple python scripts are executed via a batch file on the ECOOM calc server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"C:/Users/u0120816/Documents/Project_1/ERA5/Py/Base.py\"\n",
    "filename = \"ERA5_single_UTC-05_200601\"\n",
    "utc = 'UTC-' + filename.split(\"_\")[-2].split(\"-\")[-1][1:]\n",
    "\n",
    "# Packages\n",
    "import sys\n",
    "import xarray as xr \n",
    "import numpy as np\n",
    "import regionmask\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import glob\n",
    "import time\n",
    "import dask\n",
    "import math\n",
    "\n",
    "# Path\n",
    "PATH = \"C:/Users/u0120816/Documents/Project_1/ERA5/\"\n",
    "PATH_SF = \"C:/Users/u0120816/Documents/Project_1/Shapefiles/time_zones/\"\n",
    "\n",
    "# Start time\n",
    "start = time.time()  \n",
    "\n",
    "# Loop over all parameters\n",
    "try:\n",
    "\n",
    "    # Open County shapefile\n",
    "    county = gpd.read_file(PATH_SF+utc+'_WGS84.shp')        \n",
    "    xmin, ymin, xmax, ymax = county.total_bounds \n",
    "    xmin = math.floor(xmin)-1\n",
    "    ymin= math.floor(ymin)-1\n",
    "    xmax = math.ceil(xmax)+1\n",
    "    ymax= math.ceil(ymax)+1\n",
    "\n",
    "    # Read NetCF file        \n",
    "    d = xr.open_mfdataset(PATH+\"Cleaned/\"+filename+'.nc')    \n",
    "\n",
    "    # Generate mask of County regions\n",
    "    county_mask_poly = regionmask.Regions(name = 'county_mask', numbers = list(range(0, len(county))), names = list(county.GEOID), abbrevs = list(county.GEOID),\n",
    "                                              outlines = list(county.geometry.values[i] for i in range(0, len(county))))\n",
    "\n",
    "    # Calcutes the County mask for the ECWMF dataset\n",
    "    mask = county_mask_poly.mask(d.isel(time = 0).sel(latitude = slice(ymin, ymax), longitude = slice(xmin, xmax)), lat_name='latitude', lon_name='longitude')\n",
    "\n",
    "    # Generate empty dask dataframe (via pandas dataframe)\n",
    "    # Dataframe will be filled with data in the following loop\n",
    "    df = pd.DataFrame([])\n",
    "\n",
    "    # Calculate variables for remaining US Counties (3232 counties) and append dataframe\n",
    "    for i in range(0, len(county)):\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Select longitude and latidue where its queal to target NUTS region\n",
    "            lat = mask.latitude.values\n",
    "            lon = mask.longitude.values\n",
    "            sel_mask = mask.where(mask == i).values\n",
    "            id_lon = lon[np.where(~np.all(np.isnan(sel_mask), axis=0))]\n",
    "            id_lat = lat[np.where(~np.all(np.isnan(sel_mask), axis=1))]\n",
    "            out_sel = d.sel(latitude = slice(id_lat[0], id_lat[-1]), longitude = slice(id_lon[0], id_lon[-1])).compute().where(mask == i)\n",
    "\n",
    "            # Generate mean over counties\n",
    "            xloop = out_sel.groupby('time').mean(...)\n",
    "\n",
    "            # To pandas dataframe\n",
    "            append = xloop.to_dataframe().reset_index()\n",
    "\n",
    "            # Add GEOID and Date to dataframe\n",
    "            append['GEOID'] = county.GEOID[i]  \n",
    "\n",
    "            # Append existing dataframe\n",
    "            df = df.append(append)                \n",
    "\n",
    "        except:\n",
    "            \n",
    "            continue\n",
    "\n",
    "\n",
    "    # Export dataframe to CSV (; seperator)\n",
    "    df.to_csv(PATH+'CSV/'+filename+'.csv', columns=['time', 'blh', 'tcc', 'GEOID'], encoding='utf-8', header = [\"time\", \"boundary_layer_height\", 'cloud_cover', \"GEOID\"], index=False, sep=';', float_format='%.15f')\n",
    "\n",
    "except:\n",
    "\n",
    "    print('File not available:' + filename)\n",
    "    \n",
    "    \n",
    "print('Done!')\n",
    "end = time.time()\n",
    "print('Total Time: {} min'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Aggregate csv files to one csv file covering daily data from ERA5 single for all counties (accounting for respective time zones) over the entire period 2006 - 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import sys\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path (ECOOM CALC SERVER)\n",
    "PATH = \"C:/Users/u0120816/Documents/Project_1/ERA5/\"\n",
    "\n",
    "# Merge all csv files into on single csv file\n",
    "interesting_files = glob.glob(PATH+\"CSV/ERA5_single*.csv\") \n",
    "df = pd.concat((pd.read_csv(f, sep = ';', header = 0) for f in interesting_files))\n",
    "df.to_csv(PATH+\"CSV/ERA5_single_2006_2020.csv\", index=False, sep = ';')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> ERA5-Land hourly data </u>\n",
    "**[ERA5-Land hourly data](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land?tab=overview) from 1981 to present**. We will use **daily data** by averaging hourly data with a horizontal resolution of **0.1°x0.1°** for the period **2000 to 2019**. Specifically, the following weather variables are imported:\n",
    "* 2m temperature in K\n",
    "* 2m dewpoint temperature (used to calc relative humidity)\n",
    "* 10m u-component of wind in m s^-1\n",
    "* 10m v-component of wind in m s^-1\n",
    "* Surface pressure in Pa\n",
    "* Total precipitation in m (see \"Special Case\" below)\n",
    "\n",
    "Following code downloads ERA5 daily averages for each US time zone (UTC with respective coordinate window) though the ERA5 API. Each netCDF file contains data for one variable per month and UTC coordinate window shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import cdsapi\n",
    "import requests\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# PATH\n",
    "PATH = \"C:/Users/u0120816/OneDrive - KU Leuven/FB/Data/Python/ERA5_Land/\"\n",
    " \n",
    "# Requires:\n",
    "# 1) the CDS API to be installed and working on your system\n",
    "# 2) You have agreed to the ERA5 Licence (via the CDS web page)\n",
    "# 3) Selection of required variable, daily statistic, etc\n",
    "\n",
    "# Call API\n",
    "c = cdsapi.Client(timeout=600)\n",
    "\n",
    "# Time Zones\n",
    "UTC =  [\"UTC-05\", \"UTC-06\", \"UTC-07\", \"UTC-08\"]\n",
    "\n",
    "# Variables\n",
    "VAR =  [\"2m_temperature\", \"2m_dewpoint_temperature\", \"10m_u_component_of_wind\", \"10m_v_component_of_wind\", \"surface_pressure\"]\n",
    "\n",
    "# Years\n",
    "YEARS =  [\n",
    " '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2014', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020',\n",
    "]\n",
    "\n",
    "# Months\n",
    "MONTHS = [\n",
    "  '01','02','03', '04', '05', '06', '07', '08', '09', '10', '10', '11', '12'\n",
    "    ]\n",
    "\n",
    "# Loop over all parameters\n",
    "for yr in YEARS:\n",
    "    for mn in MONTHS:\n",
    "        for var in VAR:\n",
    "            for utc in UTC:\n",
    "                \n",
    "                print('Running: '+yr+mn+var+utc)        \n",
    "                \n",
    "                #--- UTC-05 ---#                \n",
    "\n",
    "                if utc == \"UTC-05\":\n",
    "                    result = c.service(\n",
    "                        \"tool.toolbox.orchestrator.workflow\",\n",
    "                        params={\n",
    "                            \"realm\": \"user-apps\",\n",
    "                            \"project\": \"app-c3s-daily-era5-statistics\",\n",
    "                            \"version\": \"master\",\n",
    "                            \"kwargs\": {\n",
    "                            \"dataset\": \"reanalysis-era5-single-levels\",\n",
    "                            \"product_type\": \"reanalysis\",\n",
    "                            \"variable\": var,\n",
    "                            \"statistic\": \"daily_mean\",\n",
    "                            \"year\": yr,\n",
    "                            \"month\": mn,\n",
    "                            \"time_zone\": utc+\":0\",\n",
    "                            \"frequency\": \"1-hourly\",\n",
    "                            \"grid\": \"0.1/0.1\",                        \n",
    "                            \"area\": {\"lat\": [23,50], \"lon\": [-91,-65]}   \n",
    "                    },\n",
    "                    \"workflow_name\": \"application\"\n",
    "                })\n",
    "\n",
    "                #--- UTC-06 ---#\n",
    "\n",
    "                if utc == \"UTC-06\":\n",
    "                    result = c.service(\n",
    "                        \"tool.toolbox.orchestrator.workflow\",\n",
    "                        params={\n",
    "                            \"realm\": \"user-apps\",\n",
    "                            \"project\": \"app-c3s-daily-era5-statistics\",\n",
    "                            \"version\": \"master\",\n",
    "                            \"kwargs\": {\n",
    "                            \"dataset\": \"reanalysis-era5-single-levels\",\n",
    "                            \"product_type\": \"reanalysis\",\n",
    "                            \"variable\": var,\n",
    "                            \"statistic\": \"daily_mean\",\n",
    "                            \"year\": yr,\n",
    "                            \"month\": mn,\n",
    "                            \"time_zone\": utc+\":0\",\n",
    "                            \"frequency\": \"1-hourly\",\n",
    "                            \"grid\": \"0.1/0.1\",\n",
    "                            \"area\": {\"lat\": [24,51], \"lon\": [-106,-83]}    \n",
    "                    },\n",
    "                    \"workflow_name\": \"application\"\n",
    "                })\n",
    "\n",
    "                #--- UTC-07 ---#\n",
    "\n",
    "                if utc == \"UTC-07\":\n",
    "                    result = c.service(\n",
    "                        \"tool.toolbox.orchestrator.workflow\",\n",
    "                        params={\n",
    "                            \"realm\": \"user-apps\",\n",
    "                            \"project\": \"app-c3s-daily-era5-statistics\",\n",
    "                            \"version\": \"master\",\n",
    "                            \"kwargs\": {\n",
    "                            \"dataset\": \"reanalysis-era5-single-levels\",\n",
    "                            \"product_type\": \"reanalysis\",\n",
    "                            \"variable\": var,\n",
    "                            \"statistic\": \"daily_mean\",\n",
    "                            \"year\": yr,\n",
    "                            \"month\": mn,\n",
    "                            \"time_zone\": utc+\":0\",\n",
    "                            \"frequency\": \"1-hourly\",\n",
    "                            \"grid\": \"0.1/0.1\",\n",
    "                            \"area\": {\"lat\": [29,51], \"lon\": [-120,-99]}    \n",
    "                    },\n",
    "                    \"workflow_name\": \"application\"\n",
    "                })\n",
    "\n",
    "                #--- UTC-08 ---#\n",
    "\n",
    "                if utc == \"UTC-08\":\n",
    "                    result = c.service(\n",
    "                        \"tool.toolbox.orchestrator.workflow\",\n",
    "                        params={\n",
    "                            \"realm\": \"user-apps\",\n",
    "                            \"project\": \"app-c3s-daily-era5-statistics\",\n",
    "                            \"version\": \"master\",\n",
    "                            \"kwargs\": {\n",
    "                            \"dataset\": \"reanalysis-era5-single-levels\",\n",
    "                            \"product_type\": \"reanalysis\",\n",
    "                            \"variable\": var,\n",
    "                            \"statistic\": \"daily_mean\",\n",
    "                            \"year\": yr,\n",
    "                            \"month\": mn,\n",
    "                            \"time_zone\": utc+\":0\",\n",
    "                            \"frequency\": \"1-hourly\",\n",
    "                            \"grid\": \"0.1/0.1\",\n",
    "                            \"area\": {\"lat\": [31,51], \"lon\": [-126,-113]}    \n",
    "                    },\n",
    "                    \"workflow_name\": \"application\"\n",
    "                })\n",
    "\n",
    "                # set name of output file for each month (statistic, variable, year, month)\n",
    "                file_name = \"download_\"+utc+\"_\" + var +\"_\"+ yr + mn+ \".nc\"                       \n",
    "                location=result[0]['location']    \n",
    "                res = requests.get(location, stream = True)\n",
    "                print(\"Writing data to \" + file_name)\n",
    "                with open(PATH+file_name,'wb') as fh:\n",
    "                    for r in res.iter_content(chunk_size = 1024):\n",
    "                        fh.write(r)\n",
    "                fh.close()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all files have been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Path with downloaded files\n",
    "PATH = \"Y:/\"\n",
    "\n",
    "# Generate list of files\n",
    "files = []\n",
    "df_files = pd.DataFrame(columns=['Files'])\n",
    "for file in sorted(os.listdir(PATH)):\n",
    "    if \".nc\" in file:\n",
    "        files.append(file)    \n",
    "df_files['Files']=files\n",
    "df_files['Actual_files']=1\n",
    "\n",
    "\n",
    "# Generate full list of files that should have been downloaded\n",
    "names = []\n",
    "UTC =  [\"UTC-05\", \"UTC-06\", \"UTC-07\", \"UTC-08\"]\n",
    "VAR =  [\"2m_temperature\", \"10m_u_component_of_wind\", \"10m_v_component_of_wind\", \"surface_pressure\"]\n",
    "for y in range(2006, 2021):\n",
    "    for m in range(1, 13):\n",
    "        for var in VAR:\n",
    "            for utc in UTC:       \n",
    "                d =\"download_\"+utc+\"_\" + var +\"_\"+ str(y) + str(m).zfill(2)+ \".nc\" \n",
    "                names.append(d)\n",
    "df_names = pd.DataFrame(names,  columns=['Files'])\n",
    "df_names['Required_files']=1\n",
    "\n",
    "# Compare lists and export missing files to CSV\n",
    "df = pd.merge(df_files, df_names, on='Files', how='outer')\n",
    "missings = df[df['Actual_files'].isna()].sort_values(by=['Files'])\n",
    "missings.to_csv(PATH + \"missings.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggreagte ERA5 land data on county level\n",
    "The following script aggreates ERA5 land data over each US county taking the different time zones into account.\n",
    "\n",
    "* 1. **Aggregate raw data files** in a way that all variables stored in one file which covers weather data for one year and time-zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import sys\n",
    "import xarray as xr \n",
    "import numpy as np\n",
    "import regionmask\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "import dask\n",
    "\n",
    "# Path\n",
    "PATH_INPUT = \"Y:/\"\n",
    "PATH_OUTPUT = \"C:/Users/u0120816/Documents/Project_1/ERA5/\"\n",
    "\n",
    "# Start time\n",
    "start = time.time() \n",
    "\n",
    "# Time Zones\n",
    "UTC =  [\"UTC-05\", \"UTC-06\", \"UTC-07\", \"UTC-08\"]\n",
    "\n",
    "# Years\n",
    "YEARS =  [\n",
    " '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2014', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020',\n",
    "]\n",
    "\n",
    "\n",
    "# Loop over all parameters\n",
    "for yr in YEARS:\n",
    "      for utc in UTC:                \n",
    "\n",
    "          # Open raw files seperate for each weather variable\n",
    "          d = xr.open_mfdataset(PATH_INPUT+\"download_\"+utc+\"*_\"+ str(yr) + \".nc\")    \n",
    "          \n",
    "          # Store all variables in one file\n",
    "          d.to_netcdf(PATH_OUTPUT+ 'ERA5_land_'+utc+\"_\"+ str(yr) + \".nc\")                                \n",
    "\n",
    "print('Done!')\n",
    "end = time.time()\n",
    "print('Total Time: {} min'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Aggregate data on county level**. Data aggregation are done by running multiple python scripts in parallel. Each python script aggregates weather data of one year. The following code generates multiple python scripts to clean data for an entire year. To do so, I first generate a python script containing the base code. Following, multiple python scripts are generated by replacing file names (e.g. ERA5_Land_UTC-01_2006). Multiple python scripts are executed via a batch file on the ECOOM calc server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"C:/Users/u0120816/Documents/Project_1/ERA5/Land/Py/Base.py\"\n",
    "filename = \"ERA5_land_UTC-05_2000\"\n",
    "utc = 'UTC-' + filename.split(\"_\")[-2].split(\"-\")[-1][1:]\n",
    "\n",
    "# Packages\n",
    "import sys\n",
    "import xarray as xr \n",
    "import numpy as np\n",
    "import regionmask\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "import dask\n",
    "import math\n",
    "\n",
    "# Path (ECOOM CALC Server)\n",
    "PATH = \"C:/Users/u0120816/Documents/Project_1/ERA5/Land/\"\n",
    "PATH_SF = \"C:/Users/u0120816/Documents/Project_1/Shapefiles/time_zones/\"\n",
    "\n",
    "# Start time\n",
    "start = time.time()  \n",
    "\n",
    "# Open County shapefile\n",
    "county = gpd.read_file(PATH_SF+utc+'_WGS84.shp')        \n",
    "xmin, ymin, xmax, ymax = county.total_bounds \n",
    "xmin = math.floor(xmin)-1\n",
    "ymin= math.floor(ymin)-1\n",
    "xmax = math.ceil(xmax)+1\n",
    "ymax= math.ceil(ymax)+1\n",
    "\n",
    "# Read NetCF file        \n",
    "d = xr.open_mfdataset(PATH+filename+'.nc') \n",
    "d = d.rename({'lon':'longitude'})\n",
    "d = d.rename({'lat':'latitude'})\n",
    "\n",
    "# Generate mask of County regions\n",
    "county_mask_poly = regionmask.Regions(name = 'county_mask', numbers = list(range(0, len(county))), names = list(county.GEOID), abbrevs = list(county.GEOID),\n",
    "                                          outlines = list(county.geometry.values[i] for i in range(0, len(county))))\n",
    "\n",
    "# Calcutes the County mask for the ECWMF dataset\n",
    "mask = county_mask_poly.mask(d.isel(time = 0).sel(latitude = slice(ymin, ymax), longitude = slice(xmin, xmax)), lat_name='latitude', lon_name='longitude')\n",
    "\n",
    "# Generate empty dask dataframe (via pandas dataframe)\n",
    "# Dataframe will be filled with data in the following loop\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# Calculate variables for remaining counties (3232 regions) and append dataframe\n",
    "for j in range(0, len(county)):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Select longitude and latidue where its queal to target county\n",
    "        lat = mask.latitude.values\n",
    "        lon = mask.longitude.values\n",
    "        sel_mask = mask.where(mask == j).values\n",
    "        id_lon = lon[np.where(~np.all(np.isnan(sel_mask), axis=0))]\n",
    "        id_lat = lat[np.where(~np.all(np.isnan(sel_mask), axis=1))]\n",
    "        out_sel = d.sel(latitude = slice(id_lat[0], id_lat[-1]), longitude = slice(id_lon[0], id_lon[-1])).compute().where(mask == j)\n",
    "\n",
    "        # Generate mean over county\n",
    "        xloop = out_sel.groupby('time').mean(...)\n",
    "\n",
    "        # To pandas dataframe\n",
    "        append = xloop.to_dataframe().reset_index()\n",
    "\n",
    "        # Add GEOID and Date to dataframe\n",
    "        append['GEOID'] = county.GEOID[j]  \n",
    "\n",
    "        # Append existing dataframe\n",
    "        df = df.append(append)                 \n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Export dataframe to CSV (; seperator)        \n",
    "df.to_csv(PATH+'CSV/'+filename+'.csv', columns=['time', 'sp', 't2m', 'd2m', 'u10', 'v10', 'GEOID'], encoding='utf-8', header = [\"time\", \"surface_air_pressure_in_pa\", \"temperature_in_k\", \"dewpoint_temperature_in_k\", \"eastward_wind_in_m_s\", \"northward_wind_in_m_s\", \"GEOID\"], index=False, sep=';', float_format='%.15f')\n",
    "\n",
    "print('Done!')\n",
    "end = time.time()\n",
    "print('Total Time: {} min'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Aggregate csv files to one csv file covering daily data from ERA5 single for all counties (accounting for respective time zones) over the entire period 2006 - 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import sys\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path (ECOOM CALC Server)\n",
    "PATH = \"C:/Users/u0120816/Documents/Project_1/ERA5/\"\n",
    "\n",
    "# Merge all csv files into on single csv file\n",
    "interesting_files = glob.glob(PATH+\"CSV/ERA5_land*.csv\") \n",
    "df = pd.concat((pd.read_csv(f, sep = ';', header = 0) for f in interesting_files))\n",
    "df.to_csv(PATH+\"CSV/ERA5_land_2006_2020.csv\", index=False, sep = ';')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special-Case: Total precipitation\n",
    "\n",
    "ERA5-Land records total precipitation as [accumulated variable](https://confluence.ecmwf.int/display/CKB/ERA5-Land%3A+data+documentation#heading-Accumulations) meaning that total precipitation are aggregated over a day starting from 01-UTC in $d$ to 24-UTC in $d+1$. This means the value at $d+1$ will be the **total precipitation in $d$** corresponding to **UTC+0**. Unfortunately, the accumulated precipitation values cannot be simply transfered to a different time zone since the accumulation period refers to UTC+0. Instead, the daily total precipitation for specific time zone will be caluclated in the following way:\n",
    "\n",
    "1. Transform accumulated hourly estimated to the total precipitation for an hour using the following formula (see [ECMWF website](https://confluence.ecmwf.int/pages/viewpage.action?pageId=197702790)):\n",
    "\\begin{array}{ll}\n",
    "    \\text{tp}_{h}\\ [\\text{m}] \\cdot 1000 & h = 01 \\text {UTC} \\\\\n",
    "    (\\text{tp}_{h}\\ [\\text{m}]\\ -\\ \\text{tp}_{h-1}\\ [\\text{m}])\\ \\cdot 1000 & \\text{otherwise} \\\\\n",
    "\\end{array}\n",
    "\n",
    "2. Shift UTC time zones for the total precipitation for an hour (instead of accumulated hourly precipitation!).\n",
    "3. Aggregate total precipitation for an hour to daily total precipitation.\n",
    "\n",
    "Follwoing code imports accumulated hourly precipitation from ERA5-Land:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import cdsapi\n",
    "import requests\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# PATH\n",
    "PATH = \"C:/Users/u0120816/OneDrive - KU Leuven/FB/Data/Python/ERA5_land/\"\n",
    "\n",
    "# Call API\n",
    "c = cdsapi.Client(timeout=600)\n",
    "\n",
    "YEARS =  [\n",
    "      '2006', '2007', '2008', '2009', '2010',\n",
    "]\n",
    "\n",
    "# Loop over all parameters\n",
    "for yr in YEARS:\n",
    "    result = c.retrieve(\n",
    "        'reanalysis-era5-land',\n",
    "    {\n",
    "        'variable': 'total_precipitation',\n",
    "        'year': yr ,\n",
    "        'month': [\n",
    "            '01', '02', '03',\n",
    "            '04', '05', '06',\n",
    "            '07', '08', '09',\n",
    "            '10', '11', '12',\n",
    "        ],\n",
    "        'day': [\n",
    "            '01', '02', '03',\n",
    "            '04', '05', '06',\n",
    "            '07', '08', '09',\n",
    "            '10', '11', '12',\n",
    "            '13', '14', '15',\n",
    "            '16', '17', '18',\n",
    "            '19', '20', '21',\n",
    "            '22', '23', '24',\n",
    "            '25', '26', '27',\n",
    "            '28', '29', '30',\n",
    "            '31',\n",
    "        ],\n",
    "        'time': [\n",
    "            '00:00', '01:00', '02:00',\n",
    "            '03:00', '04:00', '05:00',\n",
    "            '06:00', '07:00', '08:00',\n",
    "            '09:00', '10:00', '11:00',\n",
    "            '12:00', '13:00', '14:00',\n",
    "            '15:00', '16:00', '17:00',\n",
    "            '18:00', '19:00', '20:00',\n",
    "            '21:00', '22:00', '23:00',\n",
    "        ],\n",
    "        'area': [\n",
    "            51, -126, 23, -65,\n",
    "        ],\n",
    "    'format': 'netcdf',\n",
    "    })\n",
    "         \n",
    "# set name of output file for each month (statistic, variable, year, month)\n",
    "file_name = \"download_\"+yr+\".nc\"          \n",
    "res = requests.get(result.location, stream = True)\n",
    "print(\"Writing data to \" + file_name)\n",
    "with open(PATH+file_name,'wb') as fh:\n",
    "    for r in res.iter_content(chunk_size = 1024):\n",
    "        fh.write(r)\n",
    "fh.close()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split files by year but including the first day (01.01) of the following year. This is necessary since precipitation are aggregated over a day starting from 01-UTC in $d$ to 24-UTC in $d+1$. Thus, first observation 24-UTC in $d+1$ are required to calculate total precipitation in $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import sys\n",
    "import xarray as xr \n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import glob\n",
    "import dask\n",
    "from datetime import timedelta\n",
    "\n",
    "# PATH\n",
    "PATH = \"C:/Users/u0120816/OneDrive - KU Leuven/FB/Data/Python/ERA5_land/\"\n",
    "\n",
    "# Load ERA5 precipitation\n",
    "ds = xr.open_mfdataset(PATH+\"precipitation_*.nc\", chunks={'time': 1000})\n",
    "\n",
    "# Extract years to list\n",
    "year_list = ds['time.year']\n",
    "year_list = year_list.values.tolist()\n",
    "year_list= list(set(year_list))\n",
    "year_list = sorted(year_list)[:-1]\n",
    "\n",
    "#--- Split files by year ---#\n",
    "# Add last month of subsequnt year since last day is needed to calc pecipitation for 31.12 !\n",
    "for y in year_list:\n",
    "    y_start = str(y) +'-01-01'\n",
    "    y_end = y +1\n",
    "    y_end= str(y_end) +'-01-01'    \n",
    "    d_select= ds.sel(time=slice(y_start, y_end), drop=True)\n",
    "    d_select.to_netcdf(PATH+'Cleaned/precipitation_'+str(y)+ \".nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfrom ***accumulated precipitation*** for an hour to ***total precipitation*** for an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import sys\n",
    "import xarray as xr \n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import glob\n",
    "import dask\n",
    "from datetime import timedelta\n",
    "\n",
    "# PATH\n",
    "PATH = \"C:/Users/u0120816/OneDrive - KU Leuven/FB/Data/Python/ERA5_land/\"\n",
    "\n",
    "# Load ERA5 precipitation for one year (+ first day (01.01) of upcoming year)\n",
    "ds = xr.open_mfdataset(PATH+\"download_precipitation_2001.nc\" )\n",
    "\n",
    "\n",
    "#--- Calculate all values for h != 1 using formula listed above ---#\n",
    "\n",
    "# Generate a dataset for tp(h-1) by shifting time\n",
    "ds_subtr = ds.copy()\n",
    "ds_subtr['time'] = ds_subtr.time.get_index('time') + timedelta(hours=1)\n",
    "\n",
    "# Drop first time observation for tp(h) and last time observation for t(h-1) that doe not align due to time shift\n",
    "ds = ds.where(ds['time'] != ds.time.min(), drop=True)\n",
    "ds_subtr = ds_subtr.where(ds_subtr['time'] != ds_subtr.time.max(), drop=True)\n",
    "\n",
    "# Calculate tp(h) - tp(h-1)\n",
    "ds_h_not_01 = ds - ds_subtr\n",
    "\n",
    "# Drop where h = 01UTC\n",
    "ds_h_not_01 = ds_h_not_01.where(ds_h_not_01['time.hour'] != 1, drop=True)\n",
    "\n",
    "\n",
    "#--- Calculate all values for h = 1 using formula listed above ---#\n",
    "\n",
    "ds_h01= ds.where(ds['time.hour'] == 1, drop=True)\n",
    "\n",
    "\n",
    "#--- Generate final data by adding h = 1 to h != 1 ---#\n",
    "ds_results = xr.merge([ds_h_not_01, ds_h01], join=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split files into different time zones and calculate daily total precipitation for respective time zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract years to list\n",
    "year_list = ds_results['time.year']\n",
    "year_list = year_list.values.tolist()\n",
    "year_list= list(set(year_list))\n",
    "# Delete last year from list since daily averages will be incomplete for last year due to time shift\n",
    "year_list = sorted(year_list)[:-1]\n",
    "\n",
    "# Split file into different \"coordinate time zone windows\"\n",
    "d_UTC5 = ds_results.sel(longitude=slice(-91, -65), latitude=slice(50, 23), drop=True)\n",
    "d_UTC6 = ds_results.sel(longitude=slice(-106, -83), latitude=slice(51, 24), drop=True)\n",
    "d_UTC7 = ds_results.sel(longitude=slice(-120, -99), latitude=slice(51, 29), drop=True)\n",
    "d_UTC8 = ds_results.sel(longitude=slice(-126, -113), latitude=slice(51, 31), drop=True)\n",
    "\n",
    "# Shift time to UTC-5, UTC-6, UTC-7, UTC-8 \n",
    "# Time shift requires data from 1. January of the next year!\n",
    "d_UTC5['time'] = d_UTC5.time.get_index('time') + timedelta(hours=-5)\n",
    "d_UTC6['time'] = d_UTC6.time.get_index('time') + timedelta(hours=-6)\n",
    "d_UTC7['time'] = d_UTC7.time.get_index('time') + timedelta(hours=-7)\n",
    "d_UTC8['time'] = d_UTC8.time.get_index('time') + timedelta(hours=-8)\n",
    "\n",
    "# Calc daily averages and store as new file\n",
    "zones = [\"UTC5\", \"UTC6\", \"UTC7\", \"UTC8\"]\n",
    "for t in zones:    \n",
    "    d_select = locals()[f'd_{t}'].resample(time='D', closed='right').sum('time')\n",
    "    for y in year_list:        \n",
    "        d_save= d_select.where(d_select['time.year'] == y, drop=True)        \n",
    "        d_save.to_netcdf('Y:/ERA5_Land_precip_'+t+'_'+str(y)+'.nc')\n",
    "        print('Done! '+ t+'_'+str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregate data on county level**. Data aggregation are done by running multiple python scripts in parallel. Each python script aggregates weather data of one year. The following code generates multiple python scripts to clean data for an entire year. To do so, I first generate a python script containing the base code. Following, multiple python scripts are generated by replacing file names (e.g. ERA5_Land_precip_UTC5_2006). Multiple python scripts are executed via a batch file on the ECOOM calc server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"C:/Users/u0120816/Documents/Project_1/ERA5/Py/Base.py\"\n",
    "filename = \"ERA5_Land_precip_UTC5_2006\"\n",
    "utc = 'UTC-' + filename.split(\"_\")[-2][3:]\n",
    "\n",
    "# Packages\n",
    "import sys\n",
    "import xarray as xr \n",
    "import numpy as np\n",
    "import regionmask\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "import dask\n",
    "import math\n",
    "\n",
    "# Path (ECOOM CALC Server)\n",
    "PATH = \"C:/Users/u0120816/Documents/Project_1/ERA5/\"\n",
    "PATH_SF = \"C:/Users/u0120816/Documents/Project_1/Shapefiles/time_zones/\"\n",
    "\n",
    "# Start time\n",
    "start = time.time()  \n",
    "\n",
    "# Open County shapefile\n",
    "county = gpd.read_file(PATH_SF+utc+'_WGS84.shp')        \n",
    "xmin, ymin, xmax, ymax = county.total_bounds \n",
    "xmin = math.floor(xmin)-1\n",
    "ymin= math.floor(ymin)-1\n",
    "xmax = math.ceil(xmax)+1\n",
    "ymax= math.ceil(ymax)+1\n",
    "\n",
    "# Read NetCF file        \n",
    "d = xr.open_mfdataset(PATH+'Cleaned/'+filename+'.nc') \n",
    "\n",
    "# Generate mask of County regions\n",
    "county_mask_poly = regionmask.Regions(name = 'county_mask', numbers = list(range(0, len(county))), names = list(county.GEOID), abbrevs = list(county.GEOID),\n",
    "                                          outlines = list(county.geometry.values[i] for i in range(0, len(county))))\n",
    "\n",
    "# Calcutes the County mask for the ECWMF dataset (! switch latitude = slice(ymin, ymax) to latitude = slice(ymax, ymin) becuase manual download causes lat from max to min but API downlaod lists min to max !)\n",
    "mask = county_mask_poly.mask(d.isel(time = 0).sel(latitude = slice(ymax, ymin), longitude = slice(xmin, xmax)), lat_name='latitude', lon_name='longitude')\n",
    "\n",
    "# Generate empty dask dataframe (via pandas dataframe)\n",
    "# Dataframe will be filled with data in the following loop\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# Calculate variables for remaining counties (3232 regions) and append dataframe\n",
    "for j in range(0, len(county)):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Select longitude and latidue where its queal to target county\n",
    "        lat = mask.latitude.values\n",
    "        lon = mask.longitude.values\n",
    "        sel_mask = mask.where(mask == j).values\n",
    "        id_lon = lon[np.where(~np.all(np.isnan(sel_mask), axis=0))]\n",
    "        id_lat = lat[np.where(~np.all(np.isnan(sel_mask), axis=1))]\n",
    "        out_sel = d.sel(latitude = slice(id_lat[0], id_lat[-1]), longitude = slice(id_lon[0], id_lon[-1])).compute().where(mask == j)\n",
    "\n",
    "        # Generate mean over county\n",
    "        xloop = out_sel.groupby('time').mean(...)\n",
    "\n",
    "        # To pandas dataframe\n",
    "        append = xloop.to_dataframe().reset_index()\n",
    "\n",
    "        # Add GEOID and Date to dataframe\n",
    "        append['GEOID'] = county.GEOID[j]  \n",
    "\n",
    "        # Append existing dataframe\n",
    "        df = df.append(append)                 \n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Export dataframe to CSV (; seperator)        \n",
    "df.to_csv(PATH+'CSV/'+filename+'.csv', columns=['time', 'tp', 'GEOID'], encoding='utf-8', header = [\"time\", \"total_precipitation_m\", \"GEOID\"], index=False, sep=';', float_format='%.15f')\n",
    "\n",
    "print('Done!')\n",
    "end = time.time()\n",
    "print('Total Time: {} min'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate multiple python scripts that can be executed with an batch file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Path (ECOOM Calc Server)\n",
    "PATH = \"C:/Users/u0120816/Documents/Project_1/ERA5/Py/\"\n",
    "\n",
    "# Open python code\n",
    "with open(PATH + \"Base.py\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "#--- Change filename to be processed ---#\n",
    "# Change month in filename\n",
    "\n",
    "# Time Zones\n",
    "UTC =  [\"UTC5\", \"UTC6\", \"UTC7\", \"UTC8\"]\n",
    "\n",
    "# Loop over all parameters\n",
    "for yr in range(2006, 2011):  \n",
    "    for utc in UTC:\n",
    "\n",
    "        lines\n",
    "        # Filename given in first file of pyhton script\n",
    "        lines[0] = 'filename = '+\"'ERA5_Land_precip_\"+utc+'_'+str(yr) + \"'\\n\"\n",
    "        lines\n",
    "\n",
    "        with open(PATH + \"ERA5_\"+utc+'_'+str(yr)+\".py\", \"w\") as f:\n",
    "            f.writelines(lines)\n",
    "\n",
    "        w = open(PATH+'Filenames.txt', \"a\")\n",
    "        w.write(\"start /B python \\\"C:\\\\Users\\\\u0120816\\\\Documents\\\\Project_1\\\\ERA5\\\\Py\\\\ERA5_\"+utc+'_'+str(yr) +\".py\\\" &\" + '\\n')\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> ERA5 hourly (on pressure levels) data </u>\n",
    "**[ERA5 hourly data on pressure levels](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-pressure-levels?tab=overview) on pressure levels**. I will use daily data by averaging hourly (UTC-time) data with a horizontal resolution of 0.25°x0.25° for the period 2006 to 2019 on 37 pressure levels. Specifically, the following weather variables are imported:\n",
    "* Temperature in K\n",
    "\n",
    "Following code downloads ERA5 daily averages for each US time zone (UTC with respective coordinate window) though the ERA5 API. Each netCDF file contains data for one variable per month and UTC coordinate window shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import cdsapi\n",
    "import requests\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# PATH\n",
    "PATH = \"C:/Users/u0120816/OneDrive - KU Leuven/FB/Data/Python/ERA5_pressure_levels/\"\n",
    "\n",
    "# Call API\n",
    "c = cdsapi.Client(timeout=600)\n",
    "\n",
    "# Years\n",
    "YEARS =  [\n",
    "      '2006', '2007', '2008',\n",
    "      '2009', '2010', '2011',\n",
    "      '2012', '2013', '2014',\n",
    "      '2015', '2016', '2017',\n",
    "      '2018', '2019', '2020',\n",
    "]\n",
    "\n",
    "# Months\n",
    "MONTHS = [\n",
    "    '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'\n",
    "    ]\n",
    "\n",
    "# Loop over all parameters\n",
    "for yr in YEARS:\n",
    "    for mn in MONTHS:\n",
    "        c.retrieve(\n",
    "    'reanalysis-era5-pressure-levels',\n",
    "    {\n",
    "        'product_type': 'reanalysis',\n",
    "        'variable': ['temperature', 'relative_humidity'],\n",
    "        'pressure_level': [\n",
    "            '1', '2', '3',\n",
    "            '5', '7', '10',\n",
    "            '20', '30', '50',\n",
    "            '70', '100', '125',\n",
    "            '150', '175', '200',\n",
    "            '225', '250', '300',\n",
    "            '350', '400', '450',\n",
    "            '500', '550', '600',\n",
    "            '650', '700', '750',\n",
    "            '775', '800', '825',\n",
    "            '850', '875', '900',\n",
    "            '925', '950', '975',\n",
    "            '1000',\n",
    "        ],\n",
    "        'year': '2006',\n",
    "        'month': [\n",
    "            '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'\n",
    "        ],\n",
    "        'day': [\n",
    "            '01', '02', '03',\n",
    "            '04', '05', '06',\n",
    "            '07', '08', '09',\n",
    "            '10', '11', '12',\n",
    "            '13', '14', '15',\n",
    "            '16', '17', '18',\n",
    "            '19', '20', '21',\n",
    "            '22', '23', '24',\n",
    "            '25', '26', '27',\n",
    "            '28', '29', '30',\n",
    "            '31',\n",
    "        ],\n",
    "        'time': [\n",
    "            '00:00', '01:00', '02:00',\n",
    "            '03:00', '04:00', '05:00',\n",
    "            '06:00', '07:00', '08:00',\n",
    "            '09:00', '10:00', '11:00',\n",
    "            '12:00', '13:00', '14:00',\n",
    "            '15:00', '16:00', '17:00',\n",
    "            '18:00', '19:00', '20:00',\n",
    "            '21:00', '22:00', '23:00',\n",
    "        ],\n",
    "        'area': [\n",
    "            51, -126, 23,\n",
    "            -65,\n",
    "        ],\n",
    "        'format': 'netcdf',\n",
    "    })\n",
    "                   \n",
    "    # set name of output file for each month (statistic, variable, year, month)\n",
    "    file_name = \"download_\"+\"_\"+ yr + mn+ \".nc\"          \n",
    "    location=result[0]['location']    \n",
    "    res = requests.get(location, stream = True)\n",
    "    print(\"Writing data to \" + file_name)\n",
    "    with open(PATH+file_name,'wb') as fh:\n",
    "        for r in res.iter_content(chunk_size = 1024):\n",
    "            fh.write(r)\n",
    "    fh.close()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geopotential from ERA5 single level data\n",
    "ERA5 reanalysis data on pressure levels at lower levels, eg at 1000 or 850 hPa, can show atmospheric variables that are below the model terrain. This happens in regions such as the Himalayas or Andes and is due to the interpolation of data from model to pressure levels.The data which is below the surface can be masked out using the surface geopotential from ERA5 data on single level. The geopotential is the gravitational potential energy of a unit mass, at a particular location at the surface of the Earth, relative to mean sea level. It is also the amount of work that would have to be done, against the force of gravity, to lift a unit mass to that location from mean sea level. The (surface) geopotential height (orography) can be calculated by dividing the (surface) geopotential by the Earth's gravitational acceleration, g (=9.80665 m s-2 ). This parameter does not vary in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import xarray as xr \n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import glob\n",
    "import dask\n",
    "import cdsapi\n",
    "import requests\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# PATH\n",
    "PATH = \"C:/Users/u0120816/OneDrive - KU Leuven/FB/Data/Python/ERA5_single_levels/\"\n",
    "\n",
    "# Call API\n",
    "c = cdsapi.Client(timeout=600)\n",
    "\n",
    "result= c.retrieve(\n",
    "    'reanalysis-era5-single-levels',\n",
    "    {\n",
    "        'product_type': 'reanalysis',\n",
    "        'variable': 'geopotential',\n",
    "        'year': '2020',\n",
    "        'month': '02',\n",
    "        'day': '01',\n",
    "        'time': '00:00',\n",
    "        'area': [\n",
    "            51, -126, 23,\n",
    "            -65,\n",
    "        ],\n",
    "        'format': 'netcdf',\n",
    "    },)\n",
    "     \n",
    "file_name = \"ERA5_single_level_geopotential.nc\"                       \n",
    "res = requests.get(result.location, stream = True)\n",
    "print(\"Writing data to \" + file_name)\n",
    "with open(PATH+file_name,'wb') as fh:\n",
    "    for r in res.iter_content(chunk_size = 1024):\n",
    "        fh.write(r)\n",
    "fh.close()\n",
    "print('Done!')\n",
    "\n",
    "# Drop time dimension since geopotential is time-invariant\n",
    "d = xr.open_mfdataset(PATH+\"ERA5_single_level_geopotential.nc\").rename({'z':'z_ground'})\n",
    "d['z_ground'] = d['z_ground'].sel(time=\"2020-02-01\", drop=True)\n",
    "d = d.drop(\"time\")\n",
    "d.to_netcdf(PATH+\"ERA5_single_level_geopotential.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split file into different US time zones and calculate daily averages for each time zone. In addition, drop atmospheric variables below model terrain using the invariant geopotential from ERA5 single level. Any values for the geopotential in the ERA5 pressure level data  must be euqal or larger than the geopotential in the ERA5 single level. Values below the geopotential in the ERA5 single level indicate pressure levels below model terrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import sys\n",
    "import xarray as xr \n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import glob\n",
    "import dask\n",
    "from datetime import timedelta\n",
    "\n",
    "# PATH\n",
    "PATH = \"C:/Users/u0120816/OneDrive - KU Leuven/FB/Data/Python/ERA5_pressure_levels/\"\n",
    "\n",
    "# ERA5 pressure levels for the entire us (load at least two consecutive years since time shift requires data from 1. January of the next year !)\n",
    "d = xr.open_mfdataset(\"Y:/ERA5_pressure_temp*.nc\" )\n",
    "\n",
    "# Extract years to list\n",
    "year_list = d['time.year']\n",
    "year_list = year_list.values.tolist()\n",
    "year_list= list(set(year_list))\n",
    "# Delete last year from list since daily averages will be incomplete for last year due to time shift\n",
    "year_list = sorted(year_list)[:-1]\n",
    "\n",
    "# Split file into different \"coordinate time zone windows\"\n",
    "d_UTC5 = d.sel(longitude=slice(-91, -65), latitude=slice(50, 23), drop=True)\n",
    "d_UTC6 = d.sel(longitude=slice(-106, -83), latitude=slice(51, 24), drop=True)\n",
    "d_UTC7 = d.sel(longitude=slice(-120, -99), latitude=slice(51, 29), drop=True)\n",
    "d_UTC8 = d.sel(longitude=slice(-126, -113), latitude=slice(51, 31), drop=True)\n",
    "\n",
    "# Shift time to UTC-5, UTC-6, UTC-7, UTC-8  \n",
    "d_UTC5['time'] = d_UTC5.time.get_index('time') + timedelta(hours=-5)\n",
    "d_UTC6['time'] = d_UTC6.time.get_index('time') + timedelta(hours=-6)\n",
    "d_UTC7['time'] = d_UTC7.time.get_index('time') + timedelta(hours=-7)\n",
    "d_UTC8['time'] = d_UTC8.time.get_index('time') + timedelta(hours=-8)\n",
    "\n",
    "# Calc daily averages and store as new file\n",
    "zones = [\"UTC5\", \"UTC6\", \"UTC7\", \"UTC8\"]\n",
    "for t in zones:    \n",
    "    d_select = locals()[f'd_{t}'].resample(time='1D').mean('time')   \n",
    "    for y in year_list:        \n",
    "        d_save= d_select.where(d_select['time.year'] == y, drop=True)  \n",
    "\n",
    "        # Drop atmospheric variables below model terrain based on geopotential from ERA5 single levels\n",
    "        d_save = d_save.where(d_save.z > d_save.z_ground, drop=True)\n",
    "        d_save =d_save.drop_vars([\"z\",\"z_ground\"])\n",
    "\n",
    "        d_save.to_netcdf(PATH + 'ERA5_pressure_temp_'+t+'_'+str(y)+'.nc')\n",
    "        print('Done! '+ t+'_'+str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggreagte ERA5 pressure level data on county level\n",
    "The following script aggreates ERA5 hourly (on pressure levels) data over each US county taking the different time zones into account. ERA5 single level data is only available on a horizontal resolution of 0.25° x 0.25° which is to coarse to cover smaller counties. Thus, the data will be interpolated to finer grid of is 0.05°x0.05°. I will apply *bilinear interpolation* for the *temperature* and *nearest neighbor interpolation* for *relative humidity*. In a second step, the interpolated daily weather data will be aggreagted for each US county taking different time-zones into account. To do so, seperate shapefiles for each US timezone have been generated. Hourly weather data are aggregated for each time-zone-shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bcdafeeb49f4a053c91ab0e6c39574d7f06ced2ffbab453e3c39a37485b156f0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
