{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9231f047-9592-4909-9c64-0a4898027ece",
   "metadata": {},
   "source": [
    "# <u> Download mimimum and maximum temperature from ERA5 Land Data </u><a id='ERAimport'></a>\n",
    "ERA5 is the fifth generation ECMWF reanalysis for the global climate and weather for the past 4 to 7 decades published by the [Copernicus Climate Change Service](https://cds.climate.copernicus.eu/about-c3s). ERA5 provides hourly estimates for a large number of atmospheric, ocean-wave and land-surface quantities. ERA5 reanalysis combines model data with observations from across the world into a globally complete and consistent dataset using the laws of physics. Reanalysis produces data that goes several decades back in time, providing an accurate description of the climate of the past. We will use following datasets based on available weather variables:\n",
    "<br>\n",
    "<br> **[ERA5-Land hourly data](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land?tab=overview) from 1981 to present**. We will use **daily minimum and maximum temperature data** with a horizontal resolution of **0.1째x0.1째** for the period **2000 to 2019**. Specifically, the following weather variables are imported:\n",
    "* 2m temperature in K\n",
    "\n",
    "ERA5-Land hourly data are downloaded by using the [CDS Toolbox Editor](https://cds.climate.copernicus.eu/cdsapp#!/toolbox). The online editor allows to calculate daily minimum and maximum of hourly weather variables and export the data to netCDF format files. The following code has been used to **export daily data of the ERA5 Land hourly database**. Each netCDF file contains the data of one weather variable over period of 1 or 4 years:\n",
    "```python\n",
    "# CODE USED FOR THE CDS TOOLBOX\n",
    "# ERA5 Land hourly data from 1981 to present\n",
    "# Store files per year, due to size\n",
    "import cdstoolbox as ct\n",
    "\n",
    "@ct.application(title='Download data')\n",
    "@ct.output.download()\n",
    "@ct.output.download()\n",
    "def download_application():\n",
    "    count=1\n",
    "    for yr in ['2000', '2001',]:\n",
    "        for mn in [ '01', '02', '03', '04', '05', '06',\n",
    "            '07', '08', '09', '10', '11', '12',]:\n",
    "                u = ct.catalogue.retrieve(\n",
    "                 'reanalysis-era5-land',\n",
    "                 {             \n",
    "                 'variable': ['2m_temperature'],\n",
    "                 'year': yr,\n",
    "                 'month': mn,\n",
    "                 'day': [\n",
    "                    '01', '02', '03',\n",
    "                    '04', '05', '06',\n",
    "                    '07', '08', '09',\n",
    "                    '10', '11', '12',\n",
    "                    '13', '14', '15',\n",
    "                    '16', '17', '18',\n",
    "                    '19', '20', '21',\n",
    "                    '22', '23', '24',\n",
    "                    '25', '26', '27',\n",
    "                    '28', '29', '30',\n",
    "                    '31',\n",
    "                ],\n",
    "                'time': [\n",
    "                    '00:00', '01:00', '02:00',\n",
    "                    '03:00', '04:00', '05:00',\n",
    "                    '06:00', '07:00', '08:00',\n",
    "                    '09:00', '10:00', '11:00',\n",
    "                    '12:00', '13:00', '14:00',\n",
    "                    '15:00', '16:00', '17:00',\n",
    "                    '18:00', '19:00', '20:00',\n",
    "                    '21:00', '22:00', '23:00',\n",
    "                ],\n",
    "                 'area': [75, -30, 32, 50,],\n",
    "                 }\n",
    "                 )\n",
    "                day_min=ct.climate.daily_min(u,keep_attrs=True)\n",
    "                day_max=ct.climate.daily_max(u,keep_attrs=True)   \n",
    "                if count == 1:\n",
    "                     day_min_all_u=day_min\n",
    "                     day_max_all_u=day_max                       \n",
    "                else:       \n",
    "                     day_min_all_u=ct.cube.concat([day_min_all, day_min], dim='time')\n",
    "                     day_max_all_u=ct.cube.concat([day_max_all_u, day_max], dim='time') \n",
    "                count = count + 1              \n",
    "    return day_min_all_u, day_max_all_u\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96257497-fd9e-4e7c-ad40-9cf99fb05a71",
   "metadata": {},
   "source": [
    "Clean extracted files containing daily minimum and maximum temperature. Variables names have to be renamed in order to concated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c20da8-bc7d-4d67-be49-7c235c785e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peckages\n",
    "import sys\n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "# Path\n",
    "PATH = \"C:/Users/u0120816/OneDrive - KU Leuven/FB_Weather_Firm/Data/Python/ERA5_Land/\"\n",
    "\n",
    "year = [2000, 2002, 2004, 2006, 2008, 2010, 2012, 2014, 2016, 2018]  \n",
    "# Using for loop\n",
    "for i in year:\n",
    "    \n",
    "    j = i + 1    \n",
    "    \n",
    "    #--- Maximum Temperature ---#\n",
    "    \n",
    "    # Read NetCF file\n",
    "    dmax = xr.open_mfdataset(PATH+'ERA5_Land_'+str(i)+'_'+str(j)+'_maxTemp.nc')\n",
    "\n",
    "    # Rename variables\n",
    "    dmax = dmax.rename({'lon':'longitude'})\n",
    "    dmax = dmax.rename({'lat':'latitude'})\n",
    "    dmax = dmax.rename({'tas':'max_temp'})\n",
    "\n",
    "    # Convert longitude from the 0-360 range to -180,180    \n",
    "    dmax = dmax.assign_coords(longitude=(((dmax.longitude + 180) % 360) - 180)).sortby('longitude')      \n",
    "    \n",
    "    # Replace data\n",
    "    dmax.to_netcdf(PATH+'ERA5_Land_'+str(i)+'_'+str(j)+'_maxTemp2.nc') \n",
    "    \n",
    "    #--- Minimum Temperature ---#\n",
    "    \n",
    "    # Read NetCF file\n",
    "    dmin = xr.open_mfdataset(PATH+'ERA5_Land_'+str(i)+'_'+str(j)+'_minTemp.nc')\n",
    "\n",
    "    # Rename variables\n",
    "    dmin = dmin.rename({'lon':'longitude'})\n",
    "    dmin = dmin.rename({'lat':'latitude'})\n",
    "    dmin = dmin.rename({'tas':'min_temp'})\n",
    "\n",
    "    # Convert longitude from the 0-360 range to -180,180    \n",
    "    dmin = dmin.assign_coords(longitude=(((dmin.longitude + 180) % 360) - 180)).sortby('longitude')   \n",
    "    \n",
    "    # Replace data\n",
    "    dmin.to_netcdf(PATH+'ERA5_Land_'+str(i)+'_'+str(j)+'_minTemp2.nc') \n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d27e3-a6e5-4d79-b69c-0b72d0cb0b97",
   "metadata": {},
   "source": [
    "ERA5 Land daily weather data are aggregated on the NUTS-3 level and exported to csv files. Python codes are provided by [Matteo de Felice's website](http://www.matteodefelice.name/post/aggregating-gridded-data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe3fef-16cc-4346-97f1-4bad62a2756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peckages\n",
    "import sys\n",
    "import xarray as xr \n",
    "import numpy as np\n",
    "import regionmask\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "import dask\n",
    "\n",
    "# Path\n",
    "PATH = \"C:/Users/u0120816/OneDrive - KU Leuven/FB_Weather_Firm/Data/Python/ERA5_Land/\"\n",
    "PATH_SF = \"C:/Users/u0120816/OneDrive - KU Leuven/FB_Weather_Firm/Data/Python/Shapefiles/\"\n",
    "\n",
    "# Start time\n",
    "start = time.time()\n",
    "\n",
    "# Open NUTS shapefile\n",
    "nuts = gpd.read_file(PATH_SF+'NUTS_RG_01M_2021_4326_LEVL_3.shp')\n",
    "\n",
    "# Read NetCF file\n",
    "d = xr.open_mfdataset(PATH+'ERA5_Land_2008*.nc')\n",
    "\n",
    "# Generate mask of NUTS regions\n",
    "nuts_mask_poly = regionmask.Regions(name = 'nuts_mask', numbers = list(range(0,len(nuts))), names = list(nuts.NUTS_ID), abbrevs = list(nuts.NUTS_ID), outlines = list(nuts.geometry.values[i] for i in range(0,len(nuts))))\n",
    "\n",
    "# Calcutes the NUTS mask for the ECWMF dataset\n",
    "mask = nuts_mask_poly.mask(d.isel(time = 0).sel(latitude = slice(32, 75), longitude = slice(-30, 50)), lat_name='latitude', lon_name='longitude')\n",
    "\n",
    "# Generate empty dask dataframe (via pandas dataframe)\n",
    "# Dataframe will be filled with data in the following loop\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# Calculate variables for remaining NUTS3 regions (1511 regions) and append dataframe\n",
    "for i in range(0, len(nuts)):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Select longitude and latidue where its queal to target NUTS region\n",
    "        lat = mask.latitude.values\n",
    "        lon = mask.longitude.values              \n",
    "        sel_mask = mask.where(mask == i).values\n",
    "        id_lon = lon[np.where(~np.all(np.isnan(sel_mask), axis=0))]\n",
    "        id_lat = lat[np.where(~np.all(np.isnan(sel_mask), axis=1))]    \n",
    "        out_sel = d.sel(latitude = slice(id_lat[0], id_lat[-1]), longitude = slice(id_lon[0], id_lon[-1])).compute().where(mask == i)\n",
    "        \n",
    "        # Generate mean over region\n",
    "        xloop = out_sel.groupby('time').mean(...)\n",
    "        \n",
    "        # To pandas dataframe\n",
    "        append = xloop.to_dataframe().reset_index()\n",
    "\n",
    "        # Add NUTS ID and Date to dataframe\n",
    "        append['NUTS_ID'] = nuts.NUTS_ID[i]  \n",
    "\n",
    "        # Append existing dataframe from NUTS code 1\n",
    "        df = df.append(append)       \n",
    "\n",
    "      \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Export dataframe to CSV (; seperator)\n",
    "df.to_csv(PATH+\"CSV/era5_dew_temp_\"\".csv\", columns=['time', 'max_temp', 'min_temp', 'NUTS_ID'], encoding='utf-8', header = [\"date\", \"max_temp\", \"min_temp\",\"nuts_id\"], index=False, sep=';', float_format='%.15f')\n",
    "\n",
    "print('Done!')\n",
    "end = time.time()\n",
    "print('Total Time: {} min'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c773f9e2-c0f6-4706-b4f8-65ccdefcd247",
   "metadata": {},
   "source": [
    "Aggregate all csv files into on large csv file containg all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b62d4-79fc-41c1-a6e8-d8418668a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peckages\n",
    "import sys\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path\n",
    "PATH = \"C:/Users/u0120816/OneDrive - KU Leuven/FB_Weather_Firm/Data/Python/ERA5_Land/\"\n",
    "\n",
    "# Merge all csv files into on single csv file\n",
    "interesting_files = glob.glob(PATH+\"CSV/era5_temp_*.csv\") \n",
    "df = pd.concat((pd.read_csv(f, sep = ';', header = 0) for f in interesting_files))\n",
    "df.to_csv(PATH+\"CSV/ERA5_temp_2000_2019.csv\", index=False, sep = ';')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366ca53-16df-4383-ba5f-e1c05862bf2a",
   "metadata": {},
   "source": [
    "# <u> Download total precipitation from ERA5 Land Data </u><a id='ERAimport'></a>\n",
    "ERA5 is the fifth generation ECMWF reanalysis for the global climate and weather for the past 4 to 7 decades published by the [Copernicus Climate Change Service](https://cds.climate.copernicus.eu/about-c3s). ERA5 provides hourly estimates for a large number of atmospheric, ocean-wave and land-surface quantities. ERA5 reanalysis combines model data with observations from across the world into a globally complete and consistent dataset using the laws of physics. Reanalysis produces data that goes several decades back in time, providing an accurate description of the climate of the past. We will use following datasets based on available weather variables:\n",
    "<br>\n",
    "<br> **[ERA5-Land hourly data](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land?tab=overview) from 1950 to present**. We will use **total precipitation** with a horizontal resolution of **0.1째x0.1째** for the period **1980 to 2019**. Specifically, the following weather variables are imported:\n",
    "* total precipitation\n",
    "\n",
    "ERA-5-Land records precipitation as accumulated variable. Thus, the first observation in the upcoming day (d+1 00:00 UTC) refers to the total precipitation accumulated over the previous day [(see Conversion table for accumulated variables)](https://confluence.ecmwf.int/pages/viewpage.action?pageId=197702790). Daily accumulated precipitation data at 00:00 UTC are downloaded by using the [CDS Toolbox Editor](https://cds.climate.copernicus.eu/cdsapp#!/toolbox).\n",
    "\n",
    "```python\n",
    "# CODE USED FOR THE CDS TOOLBOX\n",
    "# ERA5 Land hourly data from 1981 to present\n",
    "# Store files per year, due to size\n",
    "import cdstoolbox as ct\n",
    "\n",
    "@ct.application(title='Download data')\n",
    "@ct.output.download()\n",
    "def download_application():\n",
    "    data = ct.catalogue.retrieve(\n",
    "        'reanalysis-era5-land',\n",
    "        {\n",
    "            'variable': 'total_precipitation',\n",
    "            'year': ['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019',],\n",
    "            'month': ['01','02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',],\n",
    "            'day': [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "                '13', '14', '15',\n",
    "                '16', '17', '18',\n",
    "                '19', '20', '21',\n",
    "                '22', '23', '24',\n",
    "                '25', '26', '27',\n",
    "                '28', '29', '30',\n",
    "                '31',\n",
    "            ],\n",
    "            'time': '00:00',             \n",
    "            'area': [ 75, -30, 32, 50,],\n",
    "        }\n",
    "    )\n",
    "    return data\n",
    "```\n",
    "The following code aggregates total precipitation over NUTS-3 regions by taking the mean of all obervations within the boundary of a NUTS-3 regions (***tp***). Multiple python scripts were executed on the ECOOM calc server in parallel via batch-file to speed-up the process. Each python script calculates daily total precipitation of NUTS-3 regions for one year stored in csv file. Following, yearly csv files are aggregated to one csv file storing entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99a0c2-e21b-4e72-85af-01b281633e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 2000\n",
    "\n",
    "# Peckages\n",
    "import sys\n",
    "import xarray as xr \n",
    "import numpy as np\n",
    "import regionmask\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "import dask\n",
    "from multiprocessing import Process\n",
    "\n",
    "# Path\n",
    "PATH = \"C:/Users/u0120816/Documents/Project_1/\"\n",
    "PATH_SF = \"C:/Users/u0120816/Documents/Project_1/Shapefiles/\"\n",
    "\n",
    "# Start time\n",
    "start = time.time()  \n",
    "\n",
    "# Open County shapefile\n",
    "nuts = gpd.read_file(PATH_SF+'NUTS_RG_01M_2021_4326_LEVL_3.shp')\n",
    "\n",
    "# Read NetCF file\n",
    "d = xr.open_mfdataset(PATH+'ERA5/ERA5_'+ str(filename)+'.nc')\n",
    "\n",
    "# Generate mask of NUTS regions\n",
    "nuts_mask_poly = regionmask.Regions(name = 'nuts_mask', numbers = list(range(0,1512)), names = list(nuts.NUTS_ID), abbrevs = list(nuts.NUTS_ID), outlines = list(nuts.geometry.values[i] for i in range(0,1512)))\n",
    "\n",
    "# Calcutes the NUTS mask for the ECWMF dataset\n",
    "mask = nuts_mask_poly.mask(d.isel(time = 0).sel(latitude = slice(32, 75), longitude = slice(-30, 50)), lat_name='latitude', lon_name='longitude')\n",
    "\n",
    "# Generate empty dask dataframe (via pandas dataframe)\n",
    "# Dataframe will be filled with data in the following loop\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# Calculate variables for remaining counties (3232 regions) and append dataframe\n",
    "for j in range(0, len(nuts)):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Select longitude and latidue where its queal to target county\n",
    "        lat = mask.latitude.values\n",
    "        lon = mask.longitude.values\n",
    "        sel_mask = mask.where(mask == j).values\n",
    "        id_lon = lon[np.where(~np.all(np.isnan(sel_mask), axis=0))]\n",
    "        id_lat = lat[np.where(~np.all(np.isnan(sel_mask), axis=1))]\n",
    "        out_sel = d.sel(latitude = slice(id_lat[0], id_lat[-1]), longitude = slice(id_lon[0], id_lon[-1])).compute().where(mask == j)\n",
    "\n",
    "        # Generate mean over county\n",
    "        xloop_mean = out_sel.groupby('time').mean(...)\n",
    "\n",
    "        # To pandas dataframe\n",
    "        append = xloop_mean.to_dataframe().reset_index()\n",
    "        \n",
    "        # Add GEOID and Date to dataframe\n",
    "        append['nuts_id'] = county.NUTS_ID[j]\n",
    "        \n",
    "        # Append existing dataframe\n",
    "        df = df.append(append) \n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "# Export dataframe to CSV (; seperator)        \n",
    "df.to_csv(PATH+'CSV/ERA5_precip_'+str(filename)+'.csv', columns=['time', 'tp', 'nuts_id'], encoding='utf-8', header = [\"time\", \"tp\", \"nuts_id\"], index=False, sep=';', float_format='%.15f')\n",
    "\n",
    "print('Done!')\n",
    "end = time.time()\n",
    "print('Total Time: {} min'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013de2f",
   "metadata": {},
   "source": [
    "# <u> Download dewpoint temperature from ERA5-Land data </u>\n",
    "Dewpoint temperature can be used to calc relative humidity using the following [Formula](https://bmcnoldy.rsmas.miami.edu/Humidity.html). Following code downloads ERA5 daily averages though the [ERA5 API](https://cds.climate.copernicus.eu/api-how-to). The API allows to specify the aggregation level (daily mean, max or min) (see [PDF Table 2](https://datastore.copernicus-climate.eu/documents/app-c3s-daily-era5-statistics/C3S_Application-Documentation_ERA5-daily-statistics-v2.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ebdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import cdsapi\n",
    "import requests\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# PATH\n",
    "PATH = \"C:/Users/u0120816/OneDrive - KU Leuven/FB_Weather_Firm/Data/Python/ERA5_Land/Raw/\"\n",
    " \n",
    "# Requires:\n",
    "# 1) the CDS API to be installed and working on your system\n",
    "# 2) You have agreed to the ERA5 Licence (via the CDS web page)\n",
    "# 3) Selection of required variable, daily statistic, etc\n",
    "\n",
    "# Call API\n",
    "c = cdsapi.Client(timeout=600)\n",
    "\n",
    "# Years\n",
    "YEARS =  [ \n",
    "#'2000', '2001', '2002', '2003', '2004', '2005'  '2006', '2011', '2012', '2014', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2008', '2009', '2010', \n",
    " '2007', \n",
    "]\n",
    "\n",
    "# Months '01','02','03', '04',\n",
    "MONTHS = [ '05', '06', '07', '08', '09', '10', '10', '11', '12']\n",
    "\n",
    "\n",
    "# Loop over all parameters\n",
    "for yr in YEARS:    \n",
    "    for mn in MONTHS:            \n",
    "        print('Running: '+yr+mn)    \n",
    "        \n",
    "        \n",
    "        result = c.service(\n",
    "            \"tool.toolbox.orchestrator.workflow\",\n",
    "            params={\n",
    "                \"realm\": \"user-apps\",\n",
    "                \"project\": \"app-c3s-daily-era5-statistics\",\n",
    "                \"version\": \"master\",\n",
    "                \"kwargs\": {\n",
    "                \"dataset\": \"reanalysis-era5-single-levels\",\n",
    "                \"product_type\": \"reanalysis\",\n",
    "                \"variable\": \"2m_dewpoint_temperature\",\n",
    "                \"statistic\": \"daily_mean\",\n",
    "                \"frequency\": \"1-hourly\",\n",
    "                \"year\": yr,\n",
    "                \"month\": mn, \n",
    "                \"grid\": \"0.1/0.1\",                        \n",
    "                \"area\": {\"lat\": [32,75], \"lon\": [-30,50]}   \n",
    "        },\n",
    "        \"workflow_name\": \"application\"\n",
    "        })\n",
    "\n",
    "\n",
    "        # set name of output file for each month (statistic, variable, year, month)\n",
    "        file_name = \"ERA5_Land_dew_temp_\"+ yr+mn + \".nc\"                       \n",
    "        location=result[0]['location']    \n",
    "        res = requests.get(location, stream = True)\n",
    "        print(\"Writing data to \" + file_name)\n",
    "        with open(PATH+file_name,'wb') as fh:\n",
    "            for r in res.iter_content(chunk_size = 1024):\n",
    "                fh.write(r)\n",
    "        fh.close()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c2118",
   "metadata": {},
   "source": [
    "Aggregate daily average dwepoint temperature on NUTS-3 level and store results ina csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d59f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ERA5_Land_dew_temp_2000'\n",
    "year = filename.split(\"_\")[-1]\n",
    "\n",
    "# Packages\n",
    "import sys\n",
    "import xarray as xr \n",
    "import numpy as np\n",
    "import regionmask\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "import dask\n",
    "import math\n",
    "\n",
    "# Server File Path\n",
    "PATH = \"D:/Users/Felix Bracht/ERA5_land/\"\n",
    "PATH_SF = \"D:/Users/Felix Bracht/Shapefiles/\"\n",
    "\n",
    "# Start time\n",
    "start = time.time()\n",
    "\n",
    "# Open NUTS shapefile\n",
    "nuts = gpd.read_file(PATH_SF+'NUTS_RG_01M_2021_4326_LEVL_3.shp')\n",
    "\n",
    "# Read NetCF file\n",
    "d = xr.open_mfdataset(PATH+filename+'*.nc')\n",
    "d = d.rename({'lon':'longitude'})\n",
    "d = d.rename({'lat':'latitude'})\n",
    "\n",
    "# Generate mask of NUTS regions\n",
    "nuts_mask_poly = regionmask.Regions(name = 'nuts_mask', numbers = list(range(0,len(nuts))), names = list(nuts.NUTS_ID), abbrevs = list(nuts.NUTS_ID), outlines = list(nuts.geometry.values[i] for i in range(0,len(nuts))))\n",
    "\n",
    "# Calcutes the NUTS mask for the ECWMF dataset\n",
    "mask = nuts_mask_poly.mask(d.isel(time = 0).sel(latitude = slice(32, 75), longitude = slice(-30, 50)), lat_name='latitude', lon_name='longitude')\n",
    "\n",
    "# Generate empty dask dataframe (via pandas dataframe)\n",
    "# Dataframe will be filled with data in the following loop\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# Calculate variables for remaining NUTS3 regions (1511 regions) and append dataframe\n",
    "for i in range(0, len(nuts)):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Select longitude and latidue where its queal to target NUTS region\n",
    "        lat = mask.latitude.values\n",
    "        lon = mask.longitude.values              \n",
    "        sel_mask = mask.where(mask == i).values\n",
    "        id_lon = lon[np.where(~np.all(np.isnan(sel_mask), axis=0))]\n",
    "        id_lat = lat[np.where(~np.all(np.isnan(sel_mask), axis=1))]    \n",
    "        out_sel = d.sel(latitude = slice(id_lat[0], id_lat[-1]), longitude = slice(id_lon[0], id_lon[-1])).compute().where(mask == i)\n",
    "        \n",
    "        # Generate mean over region\n",
    "        xloop = out_sel.groupby('time').mean(...)\n",
    "        \n",
    "        # To pandas dataframe\n",
    "        append = xloop.to_dataframe().reset_index()\n",
    "\n",
    "        # Add NUTS ID and Date to dataframe\n",
    "        append['NUTS_ID'] = nuts.NUTS_ID[i]  \n",
    "\n",
    "        # Append existing dataframe from NUTS code 1        \n",
    "        df= pd.concat((df, append), axis = 0)\n",
    "       \n",
    "\n",
    "      \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Export dataframe to CSV (; seperator)\n",
    "df.to_csv(PATH+\"CSV/era5_dew_temp_\"+str(year)+\".csv\", columns=['time', 'd2m', 'NUTS_ID'], encoding='utf-8', header = [\"date\", \"d2m\",\"nuts_id\"], index=False, sep=';', float_format='%.15f')\n",
    "\n",
    "print('Done!')\n",
    "end = time.time()\n",
    "print('Total Time: {} min'.format((end-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be98821f",
   "metadata": {},
   "source": [
    "Aggregate csv file into one csv file containing all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a207f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peckages\n",
    "import sys\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path\n",
    "PATH = \"C:/Users/u0120816/OneDrive - KU Leuven/FB_Weather_Firm/Data/Python/ERA5_Land/CSV\"\n",
    "\n",
    "# Merge all csv files into on single csv file\n",
    "interesting_files = glob.glob(PATH+\"era5_dew_temp_*.csv\") \n",
    "df = pd.concat((pd.read_csv(f, sep = ';', header = 0) for f in interesting_files))\n",
    "df.to_csv(PATH+\"ERA5_dew_temp_2000_2019.csv\", index=False, sep = ';')\n",
    "\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "bcdafeeb49f4a053c91ab0e6c39574d7f06ced2ffbab453e3c39a37485b156f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
